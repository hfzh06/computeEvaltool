# import os
# import time
# import torch
# import numpy as np
# import pandas as pd
# from PIL import Image
# import ray
# from torchvision.models import resnet50, ResNet50_Weights, resnet18, ResNet18_Weights
# from transformers import ViTImageProcessor, ViTForImageClassification
# from ultralytics import YOLO
# import random

# # --- é…ç½®åŒº ---
# IMAGE_PATH = "/root/cocodataset/val2017"
# # MODELS_TO_TEST = ["vit-large"] 
# # MODELS_TO_TEST = ["resnet-18"]
# MODELS_TO_TEST = ["resnet-18", "yolov10-s", "vit-large"]

# VIT_HF_ID = "google/vit-large-patch16-224-in21k"
# YOLO_WEIGHTS_ID = "yolov10s.pt"
# TEST_DURATION_SECONDS = 10 # æµ‹è¯•æ—¶é•¿
# BATCH_SIZE_LIST = [256, 32, 8]     # å°†åŸæ¥çš„ CONCURRENCY_LIST æ”¹åä¸º BATCH_SIZE_LIST ä»¥æ›´å‡†ç¡®æè¿°
# CLIENT_CONCURRENCY = 32    # ç›¸å½“äºä½ åŸæ¥çš„ nnï¼Œè¡¨ç¤ºåŒæ—¶æœ‰å¤šå°‘ä¸ªè¯·æ±‚åœ¨é£

# def load_images_from_dir(dir_path):
#     """ä»æŒ‡å®šç›®å½•åŠ è½½æ‰€æœ‰æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶"""
#     supported_extensions = {".jpg", ".jpeg", ".png", ".bmp", ".webp"}
#     if not os.path.isdir(dir_path):
#         # å¦‚æœæœ¬åœ°æ²¡æœ‰æ–‡ä»¶å¤¹ï¼Œç”Ÿæˆä¸€äº›éšæœºå™ªéŸ³å›¾ç”¨äºæµ‹è¯•ï¼Œé¿å…æŠ¥é”™
#         print(f"âš ï¸ ç›®å½• {dir_path} ä¸å­˜åœ¨ï¼Œç”Ÿæˆéšæœºå™ªéŸ³å›¾ç”¨äºæµ‹è¯•...")
#         return [Image.fromarray(np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)) for _ in range(10)]

#     image_paths = [os.path.join(dir_path, f) for f in os.listdir(dir_path)
#                    if os.path.splitext(f)[1].lower() in supported_extensions]
    
#     # é™åˆ¶åŠ è½½æ•°é‡ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸
#     if len(image_paths) > 200: 
#         image_paths = image_paths[:200]

#     print(f"âœ… ä» '{dir_path}' ç›®å½•åŠ è½½äº† {len(image_paths)} å¼ å›¾ç‰‡ã€‚")
#     return [Image.open(p).convert("RGB") for p in image_paths]

# @ray.remote(num_gpus=1)
# class BenchmarkActor:
#     def __init__(self, model_name, vit_hf_id, yolo_weights_id):
#         self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
#         self.model_name = model_name
#         self.kind = ""

#         print(f"ğŸš€ Actor initializing on GPU {torch.cuda.current_device()} | Model: {model_name}...")

#         if model_name == "resnet-18":
#             weights = ResNet18_Weights.DEFAULT
#             self.model = resnet18(weights=weights)
#             self.processor = weights.transforms()
#             self.kind = "cls_torch"
#         elif model_name == "vit-large":
#             self.model = ViTForImageClassification.from_pretrained(vit_hf_id)
#             self.processor = ViTImageProcessor.from_pretrained(vit_hf_id)
#             self.kind = "cls_hf"
#         elif model_name == "yolov10-s":
#             self.model = YOLO(yolo_weights_id)
#             self.processor = None
#             self.kind = "det"
        
#         if self.kind != "det":
#             self.model = self.model.to(self.device)
#             self.model.eval()

#         # --- Warmup (è‡³å…³é‡è¦) ---
#         # éšä¾¿é€ ä¸ªå‡æ•°æ®é¢„çƒ­ä¸€ä¸‹ï¼Œæ¶ˆé™¤é¦–æ¬¡æ¨ç†çš„åˆå§‹åŒ–å»¶è¿Ÿ
#         print(f"ğŸ”¥ Actor on GPU {torch.cuda.current_device()} warming up...")
#         dummy_img = np.zeros((224, 224, 3), dtype=np.uint8)
#         self.run_benchmark(1, dummy_img) # batch_size=1 warmup
#         print(f"âœ… Actor on GPU {torch.cuda.current_device()} ready.")


#     def run_benchmark(self, batch_size, image_array, internal_iters=10):
#         """
#         :param batch_size: ç›®æ ‡ Batch Size
#         :param image_array: å•å¼ å›¾ç‰‡çš„ numpy æ•°ç»„
#         """
#         # 1. æ¢å¤ä¸º PIL å›¾ç‰‡ (åªåšä¸€æ¬¡)
#         image = Image.fromarray(image_array)

#         # ------------------------------------------------------------------
#         # æ ¸å¿ƒä¿®æ”¹åŒºï¼šåªé¢„å¤„ç†ä¸€æ¬¡ï¼Œç„¶ååœ¨ Tensor å±‚é¢è¿›è¡Œå¤åˆ¶ (Repeat/Expand)
#         # ------------------------------------------------------------------
        
#         # === åœºæ™¯ 1: PyTorch åŸç”Ÿ (ResNet) ===
#         if self.kind == "cls_torch":
#             # é¢„å¤„ç†å•å¼ å›¾ç‰‡ -> [C, H, W]
#             tensor_one = self.processor(image)
#             # å¢åŠ  Batch ç»´åº¦ -> [1, C, H, W]
#             tensor_one = tensor_one.unsqueeze(0)
#             # åœ¨ GPU ä¸Šå¤åˆ¶ -> [Batch_Size, C, H, W]
#             # è¿™é‡Œçš„ repeat éå¸¸å¿«ï¼Œå‡ ä¹ä¸è€—æ—¶
#             x = tensor_one.repeat(batch_size, 1, 1, 1).to(self.device)

#         # === åœºæ™¯ 2: HuggingFace (ViT) ===
#         elif self.kind == "cls_hf":
#             # âŒ åŸæ¥çš„å†™æ³•ï¼ˆæ…¢ï¼‰ï¼šè®© Processor å¤„ç† 32 å¼ å›¾
#             # inputs = self.processor(images=[image]*batch_size, return_tensors="pt")
            
#             # âœ… æ–°çš„å†™æ³•ï¼ˆå¿«ï¼‰ï¼šåªå¤„ç† 1 å¼ å›¾
#             inputs_one = self.processor(images=image, return_tensors="pt")
            
#             # inputs_one['pixel_values'] å½¢çŠ¶æ˜¯ [1, 3, 224, 224]
#             # æˆ‘ä»¬åªéœ€è¦æŠŠå®ƒå¤åˆ¶æˆ [Batch_Size, 3, 224, 224]
#             pixel_values = inputs_one['pixel_values'].repeat(batch_size, 1, 1, 1)
            
#             # æ„é€ æ¨¡å‹è¾“å…¥å­—å…¸ï¼Œå¹¶ç§»åˆ° GPU
#             inputs = {'pixel_values': pixel_values.to(self.device)}

#         # === åœºæ™¯ 3: YOLO (ç‰¹æ®Šæƒ…å†µ) ===
#         elif self.kind == "det":
#             # YOLO çš„ predict æ¥å£æ¯”è¾ƒå°è£…ï¼Œéš¾ä»¥ç›´æ¥ä¼  Tensor è¿›è¡Œ batch repeat
#             # å¦‚æœä»…ä»…ä¸ºäº†æµ‹å‹ï¼Œè¿™é‡Œåªèƒ½ä¼  listï¼Œæˆ–è€…æ·±å…¥ hack YOLO å†…éƒ¨
#             # æ—¢ç„¶ä½ æ˜¯ä¸ºäº†æµ‹ ViTï¼Œè¿™é‡Œå¯ä»¥æš‚æ—¶ä¿æŒåŸæ ·ï¼Œæˆ–è€…ç”¨ list å¤åˆ¶
#             imgs = [image] * batch_size

#         # ------------------------------------------------------------------
#         # æ¨ç†é˜¶æ®µ (åŸºæœ¬ä¸å˜)
#         # ------------------------------------------------------------------
#         torch.cuda.synchronize()
#         t0 = time.perf_counter()
        
#         with torch.no_grad():
#             if self.kind == "cls_torch":
#                 _ = self.model(x)
#             elif self.kind == "cls_hf":
#                 # ViT æ¨¡å‹
#                 _ = self.model(**inputs).logits
#             elif self.kind == "det":
#                 _ = self.model.predict(source=imgs, verbose=False, device=self.device, batch=batch_size)
        
#         torch.cuda.synchronize()
#         t1 = time.perf_counter()
        
#         latency_ms = (t1 - t0) * 1000
#         total_images = batch_size * internal_iters
        
#         return {
#             "latency_ms": latency_ms,
#             "batch_size": batch_size,
#             "total_images_processed": total_images,
#             "gpu_id": torch.cuda.current_device()
#         }

#     # def run_benchmark(self, batch_size, image_array):
#     #     """
#     #     :param batch_size: ç›¸å½“äºåŸæ¥çš„ concurrencyï¼ŒæŒ‡ä¸€æ¬¡æ¨ç†å¤„ç†å¤šå°‘å¼ å›¾
#     #     :param image_array: å›¾ç‰‡çš„ numpy æ•°ç»„
#     #     """
#     #     image = Image.fromarray(image_array)

#     #     # é¢„å¤„ç†é˜¶æ®µ
#     #     if self.kind == "cls_torch":
#     #         # æ„é€  Batch
#     #         input_tensor = self.processor(image).unsqueeze(0)
#     #         if batch_size > 1:
#     #             input_tensor = input_tensor.repeat(batch_size, 1, 1, 1)
#     #         x = input_tensor.to(self.device)
            
#     #     elif self.kind == "cls_hf":
#     #         # HuggingFace Processor å¤„ç† batch
#     #         inputs = self.processor(images=[image]*batch_size, return_tensors="pt")
#     #         inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
#     #     elif self.kind == "det":
#     #         imgs = [image] * batch_size

#     #     torch.cuda.synchronize()
#     #     t0 = time.perf_counter()
        
#     #     with torch.no_grad():
#     #         if self.kind == "cls_torch":
#     #             _ = self.model(x)
#     #         elif self.kind == "cls_hf":
#     #             _ = self.model(**inputs).logits
#     #         elif self.kind == "det":
#     #             # YOLO predict
#     #             _ = self.model.predict(source=imgs, verbose=False, device=self.device, batch=batch_size)
        
#     #     torch.cuda.synchronize()
#     #     t1 = time.perf_counter()
        
#     #     latency_ms = (t1 - t0) * 1000
        
#     #     return {
#     #         "latency_ms": latency_ms,
#     #         "batch_size": batch_size,
#     #         "gpu_id": torch.cuda.current_device()
#     #     }

# if __name__ == "__main__":
#     # å¦‚æœå·²æœ‰ ray å®ä¾‹åˆ™è¿æ¥ï¼Œå¦åˆ™æ–°å»º
#     if ray.is_initialized():
#         ray.shutdown()
#     ray.init(address="auto", ignore_reinit_error=True)

#     images = load_images_from_dir(IMAGE_PATH)
#     n_gpus = int(ray.cluster_resources().get("GPU", 0))
#     print(f"âš¡ æ£€æµ‹åˆ° Ray é›†ç¾¤å…±æœ‰ {n_gpus} å¼  GPU")

#     if n_gpus == 0:
#         print("âŒ é”™è¯¯ï¼šé›†ç¾¤ä¸­æ²¡æœ‰ GPUï¼Œæ— æ³•è¿è¡Œ Benchmarkã€‚")
#         exit()

#     for model in MODELS_TO_TEST:
#         print(f"\n{'='*60}\nğŸ¤– æ¨¡å‹: {model}\n{'='*60}")

#         # 1. åˆ›å»º Actors
#         actors = [BenchmarkActor.remote(model, VIT_HF_ID, YOLO_WEIGHTS_ID) for _ in range(n_gpus)]
        
#         # ç­‰å¾…æ‰€æœ‰ Actor åˆå§‹åŒ–å®Œæˆ (åŒ…å« Warmup)
#         # æˆ‘ä»¬å¯ä»¥è°ƒç”¨ä¸€ä¸ªç®€å•çš„ ping æˆ–è€…åªè¦å¯¹è±¡åˆ›å»ºæˆåŠŸå³å¯ï¼Œè¿™é‡Œç®€å•ç­‰å¾…ä¸€ä¸‹
#         print("ç­‰å¾… Actors åˆå§‹åŒ–åŠé¢„çƒ­...")
#         time.sleep(5) 

#         for batch_size in BATCH_SIZE_LIST:
#             print(f"\nğŸ“Š é…ç½®: Batch Size (æ¯è¯·æ±‚) = {batch_size}, Client Concurrency (é£è¡Œè¯·æ±‚æ•°) = {CLIENT_CONCURRENCY}")
#             print(f"â±ï¸ æµ‹è¯•æ—¶é•¿: {TEST_DURATION_SECONDS} ç§’")

#             # çŠ¶æ€è¿½è¸ªå˜é‡
#             stats = {
#                 "server_latencies": [], # çº¯æ¨¡å‹æ¨ç†è€—æ—¶
#                 "e2e_latencies": [],    # å®¢æˆ·ç«¯æäº¤åˆ°æ”¶åˆ°çš„æ€»è€—æ—¶
#                 "total_images": 0,      # å¤„ç†çš„æ€»å›¾ç‰‡æ•°
#                 "total_requests": 0     # å¤„ç†çš„æ€»è¯·æ±‚æ•°
#             }
            
#             # æ ¸å¿ƒï¼šFuture -> Actor Index æ˜ å°„
#             # è¿™æ ·æˆ‘ä»¬æ‰èƒ½çŸ¥é“å“ªä¸ª Future å®Œæˆäº†ï¼Œå¯¹åº”çš„ Actor æ˜¯è°ï¼Œä»¥ä¾¿ç»™å®ƒæ´¾æ–°æ´»
#             future_to_actor_idx = {} 
#             futures_in_flight = []
#             submit_time_map = {} # è®°å½•æäº¤æ—¶é—´ç”¨äºè®¡ç®— E2E å»¶è¿Ÿ

#             start_time = time.perf_counter()

#             # --- 1. å¡«æ»¡åˆå§‹è¯·æ±‚æ±  (Bootstrap) ---
#             for i in range(CLIENT_CONCURRENCY):
#                 actor_idx = i % n_gpus # åˆå§‹è½®è¯¢åˆ†é…
#                 actor = actors[actor_idx]
                
#                 img_array = np.array(random.choice(images))
                
#                 submit_ts = time.perf_counter()
#                 # ä¼ é€’ batch_size
#                 fut = actor.run_benchmark.remote(batch_size, img_array)
                
#                 futures_in_flight.append(fut)
#                 future_to_actor_idx[fut] = actor_idx
#                 submit_time_map[fut] = submit_ts

#             # --- 2. å¾ªç¯å¤„ç†ç›´åˆ°æ—¶é—´ç»“æŸ ---
#             while time.perf_counter() - start_time < TEST_DURATION_SECONDS:
#                 # ç­‰å¾…è‡³å°‘ä¸€ä¸ªå®Œæˆ
#                 done_futures, futures_in_flight = ray.wait(futures_in_flight, num_returns=1)

#                 if not done_futures:
#                     continue

#                 # å¤„ç†å®Œæˆçš„ä»»åŠ¡
#                 for fut in done_futures:
#                     result = ray.get(fut)
#                     actor_idx = future_to_actor_idx.pop(fut)
#                     submit_ts = submit_time_map.pop(fut)
                    
#                     now = time.perf_counter()
#                     e2e_ms = (now - submit_ts) * 1000
                    
#                     # è®°å½•æ•°æ®
#                     stats["server_latencies"].append(result["latency_ms"])
#                     stats["e2e_latencies"].append(e2e_ms)
#                     stats["total_requests"] += 1
#                     stats["total_images"] += result["batch_size"]

#                     # ç«‹å³ç»™è¿™ä¸ªåˆšç©ºé—²ä¸‹æ¥çš„ Actor æ´¾å‘æ–°ä»»åŠ¡
#                     if time.perf_counter() - start_time < TEST_DURATION_SECONDS:
#                         new_actor = actors[actor_idx] # âœ… å…³é”®ä¿®æ­£ï¼šå¤ç”¨åŒä¸€ä¸ª Actor
#                         img_array = np.array(random.choice(images))
#                         new_submit_ts = time.perf_counter()
                        
#                         new_fut = new_actor.run_benchmark.remote(batch_size, img_array)
                        
#                         futures_in_flight.append(new_fut)
#                         future_to_actor_idx[new_fut] = actor_idx
#                         submit_time_map[new_fut] = new_submit_ts

#             # --- 3. æ”¶é›†å‰©ä½™è¿˜åœ¨é£çš„ä»»åŠ¡ç»“æœ (Optional) ---
#             # å¦‚æœä½ æƒ³è®¡ç®—è¿™äº›å‰©ä½™ä»»åŠ¡ï¼Œå¯ä»¥ç”¨ ray.getã€‚
#             # ä¸¥æ ¼çš„ Duration æµ‹è¯•é€šå¸¸å¿½ç•¥æœ€åä¸€æ‰¹æœªå®Œæˆçš„ï¼Œæˆ–è€…ç­‰å¾…å®ƒä»¬å®Œæˆã€‚
#             # è¿™é‡Œé€‰æ‹©ç­‰å¾…å®ƒä»¬å®Œæˆä»¥è·å¾—å®Œæ•´æ•°æ®ï¼š
#             if futures_in_flight:
#                 remaining_results = ray.get(futures_in_flight)
#                 now = time.perf_counter()
#                 for fut, res in zip(futures_in_flight, remaining_results):
#                     submit_ts = submit_time_map.pop(fut)
#                     e2e_ms = (now - submit_ts) * 1000
#                     stats["server_latencies"].append(res["latency_ms"])
#                     stats["e2e_latencies"].append(e2e_ms)
#                     stats["total_requests"] += 1
#                     stats["total_images"] += res["batch_size"]

#             actual_duration = time.perf_counter() - start_time

#             # --- 4. æœ€ç»ˆç»Ÿè®¡ä¸è¾“å‡º ---
#             server_lats = np.array(stats["server_latencies"])
#             e2e_lats = np.array(stats["e2e_latencies"])
            
#             tps = stats["total_images"] / actual_duration
#             rps = stats["total_requests"] / actual_duration # Requests per second

#             print(f"\nğŸ† {model} Results (Batch={batch_size}):")
#             print(f"  - Total Images Processed: {stats['total_images']}")
#             print(f"  - Actual Duration: {actual_duration:.2f} s")
#             print(f"  - Throughput (TPS): {tps:.2f} images/s")
#             print(f"  - QPS: {rps:.2f} requests/s")
#             print("-" * 30)
#             if len(server_lats) > 0:
#                 print(f"  - Server Latency (Model only):")
#                 print(f"    Avg: {np.mean(server_lats):.2f} ms")
#                 print(f"    P50: {np.percentile(server_lats, 50):.2f} ms")
#                 print(f"    P95: {np.percentile(server_lats, 95):.2f} ms")
#                 print(f"    P99: {np.percentile(server_lats, 99):.2f} ms")
#                 print(f"  - Client E2E Latency (Include Queue+Network):")
#                 print(f"    Avg: {np.mean(e2e_lats):.2f} ms")
#                 print(f"    P95: {np.percentile(e2e_lats, 95):.2f} ms")
#             print("="*60)

#     ray.shutdown()


import os
import time
import torch
import numpy as np
import pandas as pd
from PIL import Image
import ray
from torchvision.models import resnet50, ResNet50_Weights, resnet18, ResNet18_Weights
from transformers import ViTImageProcessor, ViTForImageClassification
from ultralytics import YOLO
import random

# --- é…ç½®åŒº ---
IMAGE_PATH = "/root/cocodataset/val2017"
MODELS_TO_TEST = ["resnet-18", "yolov10-s", "vit-large"]

VIT_HF_ID = "google/vit-large-patch16-224-in21k"
YOLO_WEIGHTS_ID = "yolov10s.pt"
TEST_DURATION_SECONDS = 10 # æµ‹è¯•æ—¶é•¿

# âœ… æ ¸å¿ƒä¿®æ”¹ï¼šæ¯ä¸ªæ¨¡å‹å¯¹åº”å›ºå®šçš„ batch_size å’Œä¸åŒçš„ CLIENT_CONCURRENCY
MODEL_CONFIGS = {
    "resnet-18": {
        "batch_size": 256,
        "client_concurrency_list": [64, 128]  # å¯ä»¥è°ƒæ•´è¿™ä¸ªåˆ—è¡¨
    },
    "yolov10-s": {
        "batch_size": 32,
        "client_concurrency_list": [64, 128]
    },
    "vit-large": {
        "batch_size": 8,
        "client_concurrency_list": [64, 128]
    }
}

def load_images_from_dir(dir_path):
    """ä»æŒ‡å®šç›®å½•åŠ è½½æ‰€æœ‰æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶"""
    supported_extensions = {".jpg", ".jpeg", ".png", ".bmp", ".webp"}
    if not os.path.isdir(dir_path):
        print(f"âš ï¸ ç›®å½• {dir_path} ä¸å­˜åœ¨ï¼Œç”Ÿæˆéšæœºå™ªéŸ³å›¾ç”¨äºæµ‹è¯•...")
        return [Image.fromarray(np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)) for _ in range(10)]

    image_paths = [os.path.join(dir_path, f) for f in os.listdir(dir_path)
                   if os.path.splitext(f)[1].lower() in supported_extensions]
    
    if len(image_paths) > 200: 
        image_paths = image_paths[:200]

    print(f"âœ… ä» '{dir_path}' ç›®å½•åŠ è½½äº† {len(image_paths)} å¼ å›¾ç‰‡ã€‚")
    return [Image.open(p).convert("RGB") for p in image_paths]

@ray.remote(num_gpus=1)
class BenchmarkActor:
    def __init__(self, model_name, vit_hf_id, yolo_weights_id):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model_name = model_name
        self.kind = ""

        print(f"ğŸš€ Actor initializing on GPU {torch.cuda.current_device()} | Model: {model_name}...")

        if model_name == "resnet-18":
            weights = ResNet18_Weights.DEFAULT
            self.model = resnet18(weights=weights)
            self.processor = weights.transforms()
            self.kind = "cls_torch"
        elif model_name == "vit-large":
            self.model = ViTForImageClassification.from_pretrained(vit_hf_id)
            self.processor = ViTImageProcessor.from_pretrained(vit_hf_id)
            self.kind = "cls_hf"
        elif model_name == "yolov10-s":
            self.model = YOLO(yolo_weights_id)
            self.processor = None
            self.kind = "det"
        
        if self.kind != "det":
            self.model = self.model.to(self.device)
            self.model.eval()

        print(f"ğŸ”¥ Actor on GPU {torch.cuda.current_device()} warming up...")
        dummy_img = np.zeros((224, 224, 3), dtype=np.uint8)
        self.run_benchmark(1, dummy_img)
        print(f"âœ… Actor on GPU {torch.cuda.current_device()} ready.")

    def run_benchmark(self, batch_size, image_array, internal_iters=10):
        image = Image.fromarray(image_array)
        
        if self.kind == "cls_torch":
            tensor_one = self.processor(image)
            tensor_one = tensor_one.unsqueeze(0)
            x = tensor_one.repeat(batch_size, 1, 1, 1).to(self.device)

        elif self.kind == "cls_hf":
            inputs_one = self.processor(images=image, return_tensors="pt")
            pixel_values = inputs_one['pixel_values'].repeat(batch_size, 1, 1, 1)
            inputs = {'pixel_values': pixel_values.to(self.device)}

        elif self.kind == "det":
            imgs = [image] * batch_size

        torch.cuda.synchronize()
        t0 = time.perf_counter()
        
        with torch.no_grad():
            if self.kind == "cls_torch":
                _ = self.model(x)
            elif self.kind == "cls_hf":
                _ = self.model(**inputs).logits
            elif self.kind == "det":
                _ = self.model.predict(source=imgs, verbose=False, device=self.device, batch=batch_size)
        
        torch.cuda.synchronize()
        t1 = time.perf_counter()
        
        latency_ms = (t1 - t0) * 1000
        total_images = batch_size * internal_iters
        
        return {
            "latency_ms": latency_ms,
            "batch_size": batch_size,
            "total_images_processed": total_images,
            "gpu_id": torch.cuda.current_device()
        }

if __name__ == "__main__":
    if ray.is_initialized():
        ray.shutdown()
    ray.init(address="auto", ignore_reinit_error=True)

    images = load_images_from_dir(IMAGE_PATH)
    n_gpus = int(ray.cluster_resources().get("GPU", 0))
    print(f"âš¡ æ£€æµ‹åˆ° Ray é›†ç¾¤å…±æœ‰ {n_gpus} å¼  GPU")

    if n_gpus == 0:
        print("âŒ é”™è¯¯ï¼šé›†ç¾¤ä¸­æ²¡æœ‰ GPUï¼Œæ— æ³•è¿è¡Œ Benchmarkã€‚")
        exit()

    for model in MODELS_TO_TEST:
        print(f"\n{'='*60}\nğŸ¤– æ¨¡å‹: {model}\n{'='*60}")

        # è·å–è¯¥æ¨¡å‹çš„é…ç½®
        config = MODEL_CONFIGS[model]
        batch_size = config["batch_size"]
        concurrency_list = config["client_concurrency_list"]

        # åˆ›å»º Actors
        actors = [BenchmarkActor.remote(model, VIT_HF_ID, YOLO_WEIGHTS_ID) for _ in range(n_gpus)]
        
        print("ç­‰å¾… Actors åˆå§‹åŒ–åŠé¢„çƒ­...")
        time.sleep(5) 

        # âœ… ä¿®æ”¹ï¼šéå†ä¸åŒçš„ CLIENT_CONCURRENCY
        for client_concurrency in concurrency_list:
            print(f"\nğŸ“Š é…ç½®: Batch Size = {batch_size}, Client Concurrency = {client_concurrency}")
            print(f"â±ï¸ æµ‹è¯•æ—¶é•¿: {TEST_DURATION_SECONDS} ç§’")

            stats = {
                "server_latencies": [],
                "e2e_latencies": [],
                "total_images": 0,
                "total_requests": 0
            }
            
            future_to_actor_idx = {} 
            futures_in_flight = []
            submit_time_map = {}

            start_time = time.perf_counter()

            # --- 1. å¡«æ»¡åˆå§‹è¯·æ±‚æ±  ---
            for i in range(client_concurrency):  # âœ… ä½¿ç”¨å½“å‰çš„ client_concurrency
                actor_idx = i % n_gpus
                actor = actors[actor_idx]
                
                img_array = np.array(random.choice(images))
                
                submit_ts = time.perf_counter()
                fut = actor.run_benchmark.remote(batch_size, img_array)
                
                futures_in_flight.append(fut)
                future_to_actor_idx[fut] = actor_idx
                submit_time_map[fut] = submit_ts

            # --- 2. å¾ªç¯å¤„ç†ç›´åˆ°æ—¶é—´ç»“æŸ ---
            while time.perf_counter() - start_time < TEST_DURATION_SECONDS:
                done_futures, futures_in_flight = ray.wait(futures_in_flight, num_returns=1)

                if not done_futures:
                    continue

                for fut in done_futures:
                    result = ray.get(fut)
                    actor_idx = future_to_actor_idx.pop(fut)
                    submit_ts = submit_time_map.pop(fut)
                    
                    now = time.perf_counter()
                    e2e_ms = (now - submit_ts) * 1000
                    
                    stats["server_latencies"].append(result["latency_ms"])
                    stats["e2e_latencies"].append(e2e_ms)
                    stats["total_requests"] += 1
                    stats["total_images"] += result["batch_size"]

                    if time.perf_counter() - start_time < TEST_DURATION_SECONDS:
                        new_actor = actors[actor_idx]
                        img_array = np.array(random.choice(images))
                        new_submit_ts = time.perf_counter()
                        
                        new_fut = new_actor.run_benchmark.remote(batch_size, img_array)
                        
                        futures_in_flight.append(new_fut)
                        future_to_actor_idx[new_fut] = actor_idx
                        submit_time_map[new_fut] = new_submit_ts

            # --- 3. æ”¶é›†å‰©ä½™ä»»åŠ¡ ---
            if futures_in_flight:
                remaining_results = ray.get(futures_in_flight)
                now = time.perf_counter()
                for fut, res in zip(futures_in_flight, remaining_results):
                    submit_ts = submit_time_map.pop(fut)
                    e2e_ms = (now - submit_ts) * 1000
                    stats["server_latencies"].append(res["latency_ms"])
                    stats["e2e_latencies"].append(e2e_ms)
                    stats["total_requests"] += 1
                    stats["total_images"] += res["batch_size"]

            actual_duration = time.perf_counter() - start_time

            # --- 4. æœ€ç»ˆç»Ÿè®¡ä¸è¾“å‡º ---
            server_lats = np.array(stats["server_latencies"])
            e2e_lats = np.array(stats["e2e_latencies"])
            
            tps = stats["total_images"] / actual_duration
            rps = stats["total_requests"] / actual_duration

            print(f"\nğŸ† {model} Results (Batch={batch_size}, Concurrency={client_concurrency}):")
            print(f"  - Total Images Processed: {stats['total_images']}")
            print(f"  - Actual Duration: {actual_duration:.2f} s")
            print(f"  - Throughput (TPS): {tps:.2f} images/s")
            print(f"  - QPS: {rps:.2f} requests/s")
            print("-" * 30)
            if len(server_lats) > 0:
                print(f"  - Server Latency (Model only):")
                print(f"    Avg: {np.mean(server_lats):.2f} ms")
                print(f"    P50: {np.percentile(server_lats, 50):.2f} ms")
                print(f"    P95: {np.percentile(server_lats, 95):.2f} ms")
                print(f"    P99: {np.percentile(server_lats, 99):.2f} ms")
                print(f"  - Client E2E Latency (Include Queue+Network):")
                print(f"    Avg: {np.mean(e2e_lats):.2f} ms")
                print(f"    P95: {np.percentile(e2e_lats, 95):.2f} ms")
            print("="*60)

    ray.shutdown()